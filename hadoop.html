<!DOCTYPE html>

<html lang="zh-CN">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width" name="viewport"/>
<meta content="#222" name="theme-color"/><meta content="Hexo 6.3.0" name="generator"/>
<link href="/blog/css/main.css" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" rel="stylesheet"/>
<script class="next-config" data-name="main" type="application/json">{"hostname":"hrfis.me","root":"/blog/","images":"/blog/images","scheme":"Gemini","darkmode":false,"version":"8.16.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","sidebar":"fadeInLeft"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/blog/js/config.js"></script>
<meta content="在你软件安装的目录里面，通常有一个 doc 文件夹
 在解压软件包之后，什么配置都别写，上来先启动了再说
 找到你软件的日志文件在哪里
 在日志里搜索 WARN ERROR
 
 为什么呢。因为你迟早会遇到报错，没遇到报错才不正常。如果你一次报错都没遇到，你可以去买彩票了。
 
     .red-text {
         color: red;
     }" name="description"/>
<meta content="article" property="og:type"/>
<meta content="Hadoop" property="og:title"/>
<meta content="https://hrfis.me/blog/hadoop.html" property="og:url"/>
<meta content="若凡的笔记" property="og:site_name"/>
<meta content="在你软件安装的目录里面，通常有一个 doc 文件夹
 在解压软件包之后，什么配置都别写，上来先启动了再说
 找到你软件的日志文件在哪里
 在日志里搜索 WARN ERROR
 
 为什么呢。因为你迟早会遇到报错，没遇到报错才不正常。如果你一次报错都没遇到，你可以去买彩票了。
 
     .red-text {
         color: red;
     }" property="og:description"/>
<meta content="zh_CN" property="og:locale"/>
<meta content="https://flume.apache.org/_images/DevGuide_image00.png" property="og:image"/>
<meta content="2024-04-26T09:45:00.000Z" property="article:published_time"/>
<meta content="2024-06-07T07:14:23.504Z" property="article:modified_time"/>
<meta content="Ruofan" property="article:author"/>
<meta content="summary" name="twitter:card"/>
<meta content="https://flume.apache.org/_images/DevGuide_image00.png" name="twitter:image"/>
<link href="https://hrfis.me/blog/hadoop" rel="canonical"/>
<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://hrfis.me/blog/hadoop.html","path":"/hadoop.html","title":"Hadoop"}</script>
<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hadoop | 若凡的笔记</title>
<noscript>
<link href="/blog/css/noscript.css" rel="stylesheet"/>
</noscript>
</head>
<body itemscope="" itemtype="http://schema.org/WebPage">
<div class="headband"></div>
<main class="main">
<div class="column">
<header class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
<div class="site-nav-toggle">
<div aria-label="切换导航栏" class="toggle" role="button">
<span class="toggle-line"></span>
<span class="toggle-line"></span>
<span class="toggle-line"></span>
</div>
</div>
<div class="site-meta">
<a class="brand" href="/blog/" rel="start">
<i class="logo-line"></i>
<p class="site-title">若凡的笔记</p>
<i class="logo-line"></i>
</a>
</div>
<div class="site-nav-right">
<div aria-label="搜索" class="toggle popup-trigger" role="button">
</div>
</div>
</div>
<nav class="site-nav">
<ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
</ul>
</nav>
</header>
<aside class="sidebar">
<div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
<ul class="sidebar-nav">
<li class="sidebar-nav-toc">
          文章目录
        </li>
<li class="sidebar-nav-overview">
          站点概览
        </li>
</ul>
<div class="sidebar-panel-container">
<!--noindex-->
<div class="post-toc-wrap sidebar-panel">
<div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#java-%E4%BB%A3%E7%A0%81%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.</span> <span class="nav-text">Java 代码运行环境的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-ubuntu-22.04.3"><span class="nav-number">2.</span> <span class="nav-text">安装 Ubuntu 22.04.3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C"><span class="nav-number">2.1.</span> <span class="nav-text">网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ssh"><span class="nav-number">2.2.</span> <span class="nav-text">SSH</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4"><span class="nav-number">2.3.</span> <span class="nav-text">存储空间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E5%AD%97%E4%BD%93"><span class="nav-number">2.4.</span> <span class="nav-text">中文字体</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hadoop-3.3.6"><span class="nav-number">3.</span> <span class="nav-text">Hadoop 3.3.6</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE"><span class="nav-number">3.1.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mapreduce"><span class="nav-number">3.2.</span> <span class="nav-text">MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hdfs"><span class="nav-number">3.3.</span> <span class="nav-text">HDFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#yarn"><span class="nav-number">3.4.</span> <span class="nav-text">YARN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-2"><span class="nav-number">3.4.1.</span> <span class="nav-text">配置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zookeeper"><span class="nav-number">4.</span> <span class="nav-text">ZooKeeper</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hbase-2.5.8"><span class="nav-number">5.</span> <span class="nav-text">HBase 2.5.8</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%BD%E4%BB%A4"><span class="nav-number">5.1.</span> <span class="nav-text">命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-3"><span class="nav-number">5.2.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-3.5.1"><span class="nav-number">6.</span> <span class="nav-text">Spark 3.5.1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%BD%E4%BB%A4-1"><span class="nav-number">6.1.</span> <span class="nav-text">命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-4"><span class="nav-number">6.2.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hive-4.0.0"><span class="nav-number">7.</span> <span class="nav-text">Hive 4.0.0</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%85%83%E6%95%B0%E6%8D%AE%E5%AD%98%E5%9C%A8-mysql-%E4%B8%8A"><span class="nav-number">7.1.</span> <span class="nav-text">配置元数据存在 MySQL 上</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-mysqldfsyarn-%E5%92%8C-hiveserver2%E5%86%8D%E7%94%A8-beeline-%E8%BF%9E%E6%8E%A5-hs2"><span class="nav-number">7.2.</span> <span class="nav-text">启动
MySQL、DFS、YARN 和 HiveServer2，再用 beeline 连接 HS2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%86%85%E9%83%A8%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="nav-number">7.3.</span> <span class="nav-text">创建内部分区表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%9C%89%E7%BB%93%E6%9E%84%E5%88%97%E7%9A%84%E5%86%85%E9%83%A8%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="nav-number">7.4.</span> <span class="nav-text">创建有结构列的内部分区表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%A4%96%E9%83%A8%E5%88%86%E6%A1%B6%E8%A1%A8%E5%88%B0-hbase%E7%94%A8%E4%B8%B4%E6%97%B6%E8%A1%A8%E6%95%B0%E6%8D%AE%E8%A6%86%E5%86%99"><span class="nav-number">7.5.</span> <span class="nav-text">创建外部分桶表到
HBase，用临时表数据覆写</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-5"><span class="nav-number">7.6.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flume-1.11.0"><span class="nav-number">8.</span> <span class="nav-text">Flume 1.11.0</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0"><span class="nav-number">8.1.</span> <span class="nav-text">启动参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hellonetcat-%E6%BA%90%E4%B8%8E-logger-%E6%8E%A5%E6%94%B6%E5%99%A8"><span class="nav-number">8.2.</span> <span class="nav-text">Hello（netcat 源与 logger
接收器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E5%8D%95%E4%B8%AA%E8%BF%BD%E5%8A%A0%E6%96%87%E4%BB%B6exec-%E6%BA%90%E4%B8%8E-hdfs-%E6%8E%A5%E6%94%B6%E5%99%A8"><span class="nav-number">8.3.</span> <span class="nav-text">实时监控单个追加文件（exec
源与 hdfs 接收器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E5%A4%9A%E4%B8%AA%E6%96%B0%E6%96%87%E4%BB%B6spooldir-%E6%BA%90%E4%B8%8E-hdfs-%E6%8E%A5%E6%94%B6%E5%99%A8"><span class="nav-number">8.4.</span> <span class="nav-text">实时监控多个新文件（spooldir
源与 hdfs 接收器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E5%A4%9A%E4%B8%AA%E8%BF%BD%E5%8A%A0%E6%96%87%E4%BB%B6taildir-%E6%BA%90%E4%B8%8E-hdfs-%E6%8E%A5%E6%94%B6%E5%99%A8"><span class="nav-number">8.5.</span> <span class="nav-text">实时监控多个追加文件（TAILDIR
源与 hdfs 接收器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7-mapreduce-%E7%BB%93%E6%9E%9C%E4%B8%8A%E4%BC%A0%E5%88%B0-hdfs"><span class="nav-number">8.6.</span> <span class="nav-text">监控 MapReduce 结果，上传到
HDFS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E9%93%BE%E6%8E%A5"><span class="nav-number">9.</span> <span class="nav-text">超链接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">10.</span> <span class="nav-text">持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%BD%E4%BB%A4-2"><span class="nav-number">10.1.</span> <span class="nav-text">命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E8%84%9A%E6%9C%AC"><span class="nav-number">10.2.</span> <span class="nav-text">集群脚本</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA-hadoop-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4"><span class="nav-number">11.</span> <span class="nav-text">搭建 Hadoop 高可用集群</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hdfs-ha"><span class="nav-number">12.</span> <span class="nav-text">HDFS-HA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-6"><span class="nav-number">12.1.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">12.2.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E9%85%8D%E7%BD%AE"><span class="nav-number">12.3.</span> <span class="nav-text">自动故障转移配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E5%AE%9E%E9%AA%8C"><span class="nav-number">12.4.</span> <span class="nav-text">自动故障转移实验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yarn-ha"><span class="nav-number">13.</span> <span class="nav-text">YARN-HA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-7"><span class="nav-number">13.1.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="nav-number">13.2.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-jsch-%E8%AE%A4%E8%AF%81%E5%A4%B1%E8%B4%A5"><span class="nav-number">14.</span> <span class="nav-text">解决 JSch 认证失败</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E6%9D%80%E6%8E%89-acitive-%E7%9A%84-nn-%E8%BF%87%E7%A8%8B%E4%B8%AD"><span class="nav-number">14.1.</span> <span class="nav-text">在杀掉 Acitive 的 NN 过程中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AF%E4%BB%B6%E7%89%88%E6%9C%AC"><span class="nav-number">14.2.</span> <span class="nav-text">软件版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%97%A5%E5%BF%97"><span class="nav-number">14.3.</span> <span class="nav-text">关键日志</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF"><span class="nav-number">14.4.</span> <span class="nav-text">解决思路</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E6%98%AF%E9%97%AE%E9%A2%98%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">15.</span> <span class="nav-text">不是问题的问题</span></a></li></ol></div>
</div>
<!--/noindex-->
<div class="site-overview-wrap sidebar-panel">
<div class="site-author animated" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<p class="site-author-name" itemprop="name">Ruofan</p>
<div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
<nav class="site-state">
<div class="site-state-item site-state-posts">
<a href="/blog/archives/">
<span class="site-state-item-count">32</span>
<span class="site-state-item-name">笔记</span>
</a>
</div>
<div class="site-state-item site-state-categories">
<a href="/blog/categories/">
<span class="site-state-item-count">3</span>
<span class="site-state-item-name">分类</span></a>
</div>
</nav>
</div>
</div>
</div>
</div>
</aside>
</div>
<div class="main-inner post posts-expand">
<div class="post-block">
<article class="post-content" itemscope="" itemtype="http://schema.org/Article" lang="zh-CN">
<link href="https://hrfis.me/blog/hadoop.html" itemprop="mainEntityOfPage"/>
<span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<meta content="/blog/images/avatar.gif" itemprop="image"/>
<meta content="Ruofan" itemprop="name"/>
</span>
<span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
<meta content="若凡的笔记" itemprop="name"/>
<meta content="" itemprop="description"/>
</span>
<span hidden="" itemprop="post" itemscope="" itemtype="http://schema.org/CreativeWork">
<meta content="Hadoop | 若凡的笔记" itemprop="name"/>
<meta content="" itemprop="description"/>
</span>
<header class="post-header">
<h1 class="post-title" itemprop="name headline">
          Hadoop
        </h1>
<div class="post-meta-container">
<div class="post-meta">
<span class="post-meta-item">
<span class="post-meta-item-icon">
<i class="far fa-calendar"></i>
</span>
<span class="post-meta-item-text">发表于</span>
<time datetime="2024-04-26T17:45:00+08:00" itemprop="dateCreated datePublished" title="创建时间：2024-04-26 17:45:00">2024-04-26</time>
</span>
<span class="post-meta-item">
<span class="post-meta-item-icon">
<i class="far fa-calendar-check"></i>
</span>
<span class="post-meta-item-text">更新于</span>
<time datetime="2024-06-07T15:14:23+08:00" itemprop="dateModified" title="修改时间：2024-06-07 15:14:23">2024-06-07</time>
</span>
<span class="post-meta-item">
<span class="post-meta-item-icon">
<i class="far fa-folder"></i>
</span>
<span class="post-meta-item-text">分类于</span>
<span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
<a href="/blog/p/" itemprop="url" rel="index"><span itemprop="name">实用系列</span></a>
</span>
</span>
</div>
</div>
</header>
<div class="post-body" itemprop="articleBody"><ol type="1">
<li>在你软件安装的目录里面，通常有一个 doc 文件夹</li>
<li>在解压软件包之后，什么配置都别写，上来先启动了再说</li>
<li>找到你软件的日志文件在哪里</li>
<li>在日志里搜索 <code>WARN</code> <code>ERROR</code></li>
</ol>
<p>为什么呢。因为你<span class="red-text">迟早会遇到报错，没遇到报错才不正常</span>。如果你一次报错都没遇到，你可以去买彩票了。</p>
<style>
    .red-text {
        color: red;
    }
</style>
<span id="more"></span>
<style>
    .main{
        width:100%
    }
</style>
<p>Hadoop 是一个软件，它是用 Java 写的，需要运行在 Java
虚拟机上。你以后还要通过写 Java 代码来连接它其中的一个组件，叫
HDFS。所有 Java 代码都得过一道 Java 编译器，然后在 JVM（Java
虚拟机）上运行。</p>
<p>我们为什么要装 Linux 虚拟机？</p>
<ol type="1">
<li>模拟分布式集群</li>
<li>生产环境都用 Linux</li>
</ol>
<p>按理来说：</p>
<ol type="1">
<li>Hadoop 需要 JVM 去运行</li>
<li>在 Windows 上照样可以运行 JVM</li>
<li>Hadoop 可以运行在 Windows 上</li>
</ol>
<h2 id="java-代码运行环境的问题">Java 代码运行环境的问题</h2>
<p>你只要想“连接”一个什么东西，一定会碰到这个问题。</p>
<p>“连接 HDFS”是什么意思？</p>
<p>再往前问：你是如何知道你的“集群”是正在运行着的？用 <code>jps</code>
查看 Java 进程。</p>
<p><code>jps</code> 查看的是什么？是所有正在运行着的 JVM
实例，每一个进程是一个单独的 JVM。</p>
<p>“连接 HDFS”的意思是：<strong>再运行一个新的
JVM</strong>，称为“客户端”。这个 JVM 要与另一个 JVM 通信，那个 JVM 叫
<strong>NameNode</strong>。</p>
<p>怎么通信？虽然不知道细节，但你可能看过这样的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example;</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br/><span class="line"><span class="keyword">import</span> java.net.URI;</span><br/><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br/><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br/><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br/><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">App</span> {</span><br/><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException {</span><br/><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br/><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">"hdfs://ubuntu101:9820"</span>), conf);</span><br/><span class="line">        FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"/"</span>));</span><br/><span class="line">        <span class="keyword">for</span> (FileStatus status : fileStatuses) {</span><br/><span class="line">            System.out.println(status.getPath());</span><br/><span class="line">        }</span><br/><span class="line">        fs.close();</span><br/><span class="line">    }</span><br/><span class="line">}</span><br/></pre></td></tr></table></figure>
<p>现在的问题在于：</p>
<ol type="1">
<li><b><span class="red-text">它 <code>import</code>
的东西是怎么来的？</span></b></li>
<li><b><span class="red-text">在它编译完之后，放到 JVM
上运行的时候（即“运行时”），它还需不需要它 <code>import</code>
的东西？</span></b></li>
<li><b><span class="red-text">它运行时的 JVM
在哪里，在虚拟机上还是在物理机上？如果在物理机上，它能不能与虚拟机通信？</span></b></li>
<li><b><span class="red-text">IDE 起的是什么作用？</span></b></li>
</ol>
<p>先回答第四个问题：<strong>IDE 起的是工具人的作用</strong>。如果你用的
IDEA，并且成功连接 HDFS 了，你会在 IDEA 的命令行看到：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">/lib/jvm/java-8-openjdk-amd64/bin/java -javaagent:/opt/idea-IC-241.15989.150/lib/idea_rt.jar=43273:/opt/idea-IC-241.15989.150/bin\</span><br/><span class="line">                                        -Dfile.encoding=UTF-8\</span><br/><span class="line">                                        -classpath /lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/后面全都是Jar包的路径</span><br/></pre></td></tr></table></figure>
<p>它就是执行了一条命令：打开你的 JDK 目录里的 <code>java</code>
，然后向这个东西传了几个参数：工具人、文件编码、classpath</p>
<p>关键的就是 classpath，<strong>你需要保证你代码“运行时”的东西能在
classpath 里找到</strong>。</p>
<p>再回答第二个问题：它 <code>import</code>
的东西都是它的“运行时”需要的东西吗？</p>
<p>不一定，这是它“编译时”需要的东西。当他报错报找不到某个类的时候，就是它“运行时”需要的东西。</p>
<p>再回答第一个问题：搞到这些 jar 包。</p>
<ul>
<li>用 Maven</li>
<li>如果你看过你 Hadoop 的安装目录，你会看到一个 <code>share</code>
文件夹</li>
</ul>
<p>对于第三个问题的解决思路：</p>
<ul>
<li>在虚拟机上运行客户端，连接虚拟机上的 NameNode</li>
<li>在物理机上运行客户端，连接物理机上的
NameNode（如果你物理机搞成功了）</li>
<li>在物理机上运行客户端，通过 winutils 这个中间人与虚拟机上的 NameNode
通信</li>
</ul>
<p>笔者最终的解决方案是：在虚拟机上安装 IDE，把
<code>$HADOOP_HOME/share/hadoop</code> 里的 Jar 包导进 IDE
里（这里用的是 IDEA），按播放键运行。如果报找不到类的错误，注意：在
<code>$HADOOP_HOME/share/hadoop/子文件夹</code> 下还有一个叫
<code>lib</code> 的文件夹。</p>
<h2 id="安装-ubuntu-22.04.3">安装 Ubuntu 22.04.3</h2>
<ol type="1">
<li>下载 Linux
操作系统镜像，可以理解为操作系统的“安装包”。https://launchpad.net/ubuntu/+cdmirrors</li>
<li>下载一个支持在 Windows 操作系统下运行 Linux
镜像的软件（宿主），它相当于一个没装操作系统的电脑，但是装了引导加载程序
GRUB
<ul>
<li>使用 VM Player</li>
</ul></li>
<li>在宿主里“创建两台虚拟机”，相当于对这一份镜像，安装了两个新的操作系统，运行在你的宿主和
Windows 操作系统上</li>
</ol>
<p>前面三步改用 WSL（适用于 Linux 的 Windows
子系统）https://learn.microsoft.com/zh-cn/windows/wsl/</p>
<ol start="4" type="1">
<li><p>配置网络，给虚拟机和物理机搭上鹊桥</p>
<ul>
<li>不同虚拟机采用不同静态 IP</li>
<li>把虚拟机网卡路由到宿主的网关，在
<code>C:\ProgramData\VMware\vmnetnat.conf</code> 查看 VM 的 NAT
网关地址</li>
<li>修改 <code>/etc/hostname</code> 为自定义主机名</li>
<li><code>/etc/hosts</code>
<code>C:\Windows\System32\drivers\etc\hosts</code></li>
</ul></li>
</ol>
<p>如果你用的是 WSL，你在它上面开一个端口，可以直接在物理机上使用
<code>localhost:port</code> 访问。<strong>如果你装了两台 WSL，它们的 IP
会是相同的</strong>，这个问题不好解决。据说<span class="red-text">学大数据的都找不到大数据相关工作</span>。那我们就退而求其次，根本没有必要搭分布式集群，在一台机器上搭个伪分布式就行了，文件分片只分一片。这样配置文件还不至于备份来备份去，只保留一份配置文件。</p>
<ol start="5" type="1">
<li><p>配置 Ubuntu 软件源
https://mirrors.ustc.edu.cn/help/ubuntu.html</p></li>
<li><p>安装 JDK
https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions</p></li>
<li><p>写环境变量</p></li>
<li><p>配置 SSH https://wangdoc.com/ssh/</p></li>
</ol>
<h3 id="网络">网络</h3>
<p>如果你用的 WSL + 单机伪分布式，只需要修改
<code>/etc/wsl.conf</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">[network]</span><br/><span class="line">hostname=localhost</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br/><span class="line">wsl --shutdown</span><br/><span class="line">wsl</span><br/></pre></td></tr></table></figure>
<p>如果你用的 VM + Ubuntu 22.04:</p>
<p>编辑<code>/etc/netplan</code>下的<code>00-installer-config.yaml</code>文件。<a href="https://netplan.readthedocs.io/en/stable/netplan-tutorial/" rel="noopener" target="_blank">netplan
文档</a></p>
<p>选择<code>192.168.78</code>的依据是：</p>
<ul>
<li>在物理机使用<code>ipconfig</code>命令得到的【VMnet8】的 IPv4
地址<code>192.168.78.1</code></li>
<li>查看 <code>C:\ProgramData\VMware\vmnetnat.conf</code> 里的 NAT
网关地址<code>192.168.78.2</code></li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/></pre></td><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br/><span class="line">  <span class="attr">version:</span> <span class="number">2</span></span><br/><span class="line">  <span class="attr">ethernets:</span></span><br/><span class="line">    <span class="attr">ens33:</span></span><br/><span class="line">      <span class="attr">dhcp4:</span> <span class="literal">false</span></span><br/><span class="line">      <span class="attr">dhcp6:</span> <span class="literal">false</span></span><br/><span class="line">      <span class="attr">addresses:</span></span><br/><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.78</span><span class="number">.101</span><span class="string">/24</span> <span class="comment"># 每台机器设置成不同的</span></span><br/><span class="line">      <span class="attr">routes:</span></span><br/><span class="line">        <span class="bullet">-</span> <span class="attr">to:</span> <span class="string">default</span></span><br/><span class="line">          <span class="attr">via:</span> <span class="number">192.168</span><span class="number">.78</span><span class="number">.2</span></span><br/><span class="line">      <span class="attr">nameservers:</span></span><br/><span class="line">        <span class="attr">addresses:</span></span><br/><span class="line">          <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.78</span><span class="number">.2</span></span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">sudo netplan try</span><br/></pre></td></tr></table></figure>
<h3 id="ssh">SSH</h3>
<p>如果你用的 WSL + 单机伪分布式，下面两条命令只用做一次。</p>
<p>如果你搭的分布式：每台机器上都执行：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -m PEM</span><br/></pre></td></tr></table></figure>
<p>笔者这里的版本是
<code>OpenSSH_8.9p1 Ubuntu-3ubuntu0.6, OpenSSL 3.0.2 15 Mar 2022</code>，要加上
<code>-m PEM</code>，确保私钥以 -----BEGIN <strong>RSA</strong> PRIVATE
KEY----- 开头。<a href="https://www.cnblogs.com/simple-li/p/14654812.html" rel="noopener" target="_blank">后续错误</a></p>
<p>每台机器上都执行：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">ssh-copy-id ubuntu101</span><br/><span class="line">ssh-copy-id ubuntu102</span><br/><span class="line">...</span><br/></pre></td></tr></table></figure>
<h3 id="存储空间">存储空间</h3>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">du</span> -h --max-depth=1</span><br/></pre></td></tr></table></figure>
<ul>
<li>给虚拟机扩容：<code>https://hrfis.me/blog/linux.html#扩容</code></li>
<li>使用 https://www.diskgenius.cn/ 可以把你 D 盘的空间向 C
盘匀一点。如果你有多余的恢复分区，可以把它删了，只保留一个。</li>
<li>使用 https://github.com/redtrillix/SpaceSniffer
可以可视化地展示你硬盘的占用情况。</li>
</ul>
<h3 id="中文字体">中文字体</h3>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">sudo apt install language-pack-zh-hans</span><br/><span class="line">sudo vi /etc/default/locale</span><br/><span class="line">sudo <span class="built_in">mv</span> /mnt/c/path/to/font ~ /usr/share/fonts</span><br/></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">LANG=zh_CN.UTF-8</span><br/><span class="line">LANGUAGE="zh_CN:zh:en_US:en"</span><br/></pre></td></tr></table></figure>
<h2 id="hadoop-3.3.6">Hadoop 3.3.6</h2>
<ol start="9" type="1">
<li>下载 Hadoop 软件包
<ul>
<li>在虚拟机上直接用 <code>wget</code></li>
<li>用物理机下载它，从物理机的文件系统上再转移到虚拟机的文件系统上（在你的物理机硬盘上表现为
<code>.vmdk</code> 文件）
<ul>
<li>VM Player
有共享文件夹功能，把你物理机硬盘的某一个文件夹挂载到虚拟机的
<code>/mnt/hgfs/Shared</code></li>
<li>如果使用 WSL，你的物理机硬盘会被挂载到虚拟机的
<code>/mnt</code></li>
<li>使用 XFtp 软件，与你的虚拟机进行 SSH 网络协议连接</li>
</ul></li>
</ul></li>
<li>在两台虚拟机上都把软件包解压。<code>/opt</code> 目录是空的，option
的意思，让你自己选择装不装到这里。</li>
</ol>
<h3 id="配置">配置</h3>
<ol start="11" type="1">
<li>修改它们的配置文件，要保证每台机器配置文件内容相同。使用 VSCode 的
Remote-SSH 插件可以直接修改虚拟机内的文件，如果你用的是 WSL
更方便。</li>
</ol>
<p><span class="red-text">在 $HADOOP_HOME/share/doc/hadoop/index.html
左下角有默认的配置文件。</span></p>
<ul>
<li><code>hadoop-env.sh</code> 指定 JAVA_HOME，启动 JVM 时的参数</li>
<li><code>core-site.xml</code> 指定 hdfs 的
URI，文件系统存在本地哪个目录</li>
<li><code>hdfs-site.xml</code> 指定谁当 NN、2NN，副本个数</li>
<li><code>mapred-site.xml</code> 指定 MR 框架，MR 历史服务器</li>
<li><code>yarn-site.xml</code> 指定 RM，YARN 历史服务器</li>
<li><code>workers</code> 指定谁当 DataNode</li>
</ul>
<p>勤看日志。当 CPU 占用高，写磁盘不到 1MB/s，可能是出问题了在一直写
log。</p>
<ul>
<li>我们为什么要在 <code>hadoop-env.sh</code> 里写
<code>JAVA_HOME</code>？因为没写的时候，它会报错： JAVA_HOME is not set
and could not be found；</li>
<li>我们为什么要配置 SSH？因为没配置它会报错：Could not resolve hostname
xxx: Name or service not known；</li>
<li>我们为什么要在 <code>core-site.xml</code> 里配置
<code>fs.defaultFS</code> ？因为没配置它会报错：Cannot set priority of
namenode process xxx。在日志文件里有：No services to connect, missing
NameNode address；</li>
<li>我们为什么要在 <code>core-site.xml</code> 里配置
<code>hadoop.tmp.dir</code> ？因为它默认存在 <code>/tmp</code>
文件夹下，而 <code>/tmp</code> 文件夹一重启就没了；</li>
<li>我们为什么要在 <code>hdfs-site.xml</code> 里配置
<code>dfs.namenode.http-address</code> ？因为我们想用浏览器访问
HDFS；</li>
<li>我们为什么要在 <code>core-site.xml</code> 里配置
<code>hadoop.http.staticuser.user</code>
？因为如果不配置，只有读的权限，没有写的权限；</li>
<li>我们为什么可以不在 <code>hadoop-env.sh</code>
里配置各种用户名？因为还没有遇到报错的时候</li>
</ul>
<ol start="12" type="1">
<li>在 NameNode 上执行 <code>hdfs -namenode format</code>，把 Hadoop
的文件系统 HDFS
初始化。在你的物理机文件系统上有一个虚拟机文件系统，在虚拟机文件系统上又有一个
HDFS</li>
</ol>
<h3 id="mapreduce">MapReduce</h3>
<ol type="1">
<li><code>start-dfs.sh</code> 再 <code>jps</code> 查看 JVM
进程，你会看到 NN、2NN、DN</li>
<li>不需要 <code>start-yarn.sh</code>（如果你没有指定 MR 框架为
YARN，它默认为 local）</li>
<li></li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br/><span class="line">vi 1.txt</span><br/><span class="line">hdfs dfs -put 1.txt /wcinput</span><br/><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /wcinput /wcoutput</span><br/></pre></td></tr></table></figure>
<h4 id="配置-1">配置</h4>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/opt/hadoop-3.3.6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/opt/hadoop-3.3.6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/opt/hadoop-3.3.6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/></pre></td></tr></table></figure>
<h3 id="hdfs">HDFS</h3>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs oev -p XML -i edits_xxxx -o ./edits_xxxx.xml</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs oiv -p XML -i fsimage_xxxx -o ./fsimage_xxxx.xml</span><br/></pre></td></tr></table></figure>
<p>EditLog 和 FsImage 在：</p>
<ul>
<li>NN 的 <code>${hadoop.tmp.dir}/dfs/name/current</code></li>
<li>2NN 的
<code>${hadoop.tmp.dir}/data/dfs/namesecondary/current</code></li>
</ul>
<h3 id="yarn">YARN</h3>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br/></pre></td></tr></table></figure>
<h4 id="配置-2">配置</h4>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/></pre></td></tr></table></figure>
<h2 id="zookeeper">ZooKeeper</h2>
<p>如果你只用一台机器，应该不用装。</p>
<p>ZooKeeper
特点是只要有半数以上的节点正常工作，整个集群就能正常工作，所以适合装到奇数台服务器上。</p>
<p>配置 <code>zkData/myid</code></p>
<p>配置 <code>conf/zoo.cfg</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">dataDir=/usr/local/zookeeper-3.8.2/zkData</span><br/><span class="line">server.101=master:2888:3888</span><br/><span class="line">server.102=worker1:2888:3888</span><br/></pre></td></tr></table></figure>
<h2 id="hbase-2.5.8">HBase 2.5.8</h2>
<p>它自带 ZooKeeper 3.8.3，StandAlone 模式下只有一个 HMaster
进程：其中包括 HMaster，单个 HRegionServer 和 ZooKeeper 守护进程。</p>
<p>它的数据可以存在本地或 HDFS 上，这里配置存在 HDFS 上。</p>
<p>https://hbase.apache.org/book.html#standalone_dist</p>
<p>https://hbase.apache.org/book.html#shell_exercises</p>
<h3 id="命令">命令</h3>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br/><span class="line">start-hbase.sh</span><br/><span class="line">hbase shell</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span></span><br/><span class="line">list</span><br/><span class="line">create <span class="string">'tablename'</span>,<span class="string">'cfname1'</span>,<span class="string">'cfname2'</span></span><br/><span class="line">put <span class="string">'tablename'</span>,<span class="string">'rowkey'</span>,<span class="string">'cfname1:qualifiername1'</span>,<span class="string">'value'</span></span><br/><span class="line">put <span class="string">'tablename'</span>,<span class="string">'rowkey'</span>,<span class="string">'cfname2:'</span>,<span class="string">'value'</span></span><br/><span class="line">scan <span class="string">'tablename'</span></span><br/><span class="line">scan <span class="string">'tablename'</span>,{COLUMNS=&gt;<span class="string">'cfname1'</span>}</span><br/><span class="line">get <span class="string">'tablename'</span>,<span class="string">'rowkey'</span></span><br/><span class="line">describe <span class="string">'tablename'</span></span><br/><span class="line"><span class="built_in">truncate</span> <span class="string">'tablename'</span></span><br/><span class="line">drop <span class="string">'tablename'</span></span><br/></pre></td></tr></table></figure>
<h3 id="配置-3">配置</h3>
<p>如果你没有在 <code>hbase-env.sh</code> 里配置
<code>JAVA_HOME</code>，你会看到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/></pre></td><td class="code"><pre><span class="line">127.0.0.1: +======================================================================+</span><br/><span class="line">127.0.0.1: |                    Error: JAVA_HOME is not set                       |</span><br/><span class="line">127.0.0.1: +----------------------------------------------------------------------+</span><br/><span class="line">127.0.0.1: | Please download the latest Sun JDK from the Sun Java web site        |</span><br/><span class="line">127.0.0.1: |     &gt; http://www.oracle.com/technetwork/java/javase/downloads        |</span><br/><span class="line">127.0.0.1: |                                                                      |</span><br/><span class="line">127.0.0.1: | HBase requires Java 1.8 or later.                                    |</span><br/><span class="line">127.0.0.1: +======================================================================+</span><br/></pre></td></tr></table></figure>
<p><code>hbase-site.xml</code>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9820/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.wal.provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>filesystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/></pre></td></tr></table></figure>
<p>如果不设置 zkData 目录，它会在 <code>/tmp/hbase-yourname</code>
下。</p>
<p>第三个配置项是为了解决：参见
https://hbase.apache.org/book.html#wal.providers</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">ERROR [RS-EventLoopGroup-3-2] util.NettyFutureUtils (NettyFutureUtils.java:lambda$addListener$0(58)) - Unexpected error caught when processing netty</span><br/><span class="line">java.lang.IllegalArgumentException: object is not an instance of declaring class</span><br/></pre></td></tr></table></figure>
<h2 id="spark-3.5.1">Spark 3.5.1</h2>
<h3 id="命令-1">命令</h3>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">start-master.sh</span><br/><span class="line">start-workers.sh</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master yarn <span class="variable">$SPARK_HOME</span>/examples/jars/spark-examples_2.12-3.5.1.jar</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">spark-shell --master yarn</span><br/><span class="line">val textFile = sc.textFile(<span class="string">"hdfs://localhost:9820/wcinput"</span>)</span><br/><span class="line">textFile.count()</span><br/></pre></td></tr></table></figure>
<h3 id="配置-4">配置</h3>
<p><code>spark-env.sh</code>：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/opt/hadoop-3.3.6/bin/hadoop classpath)</span><br/><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop</span><br/><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=localhost</span><br/></pre></td></tr></table></figure>
<p><code>workers</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">localhost</span><br/></pre></td></tr></table></figure>
<h2 id="hive-4.0.0">Hive 4.0.0</h2>
<p>https://developer.aliyun.com/article/632261</p>
<p>Hive 的数据默认存在 HDFS 里，元数据可以存在 MySQL 上。</p>
<p>分外部表和内部表。内部表默认存在 HDFS，外部表可以存在 HBase 里。</p>
<h3 id="配置元数据存在-mysql-上">配置元数据存在 MySQL 上</h3>
<p>https://www.mysqltutorial.org/getting-started-with-mysql/install-mysql-ubuntu/</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">sudo systemctl start mysql.service</span><br/><span class="line">mysql -u root -p</span><br/><span class="line">create database hive;</span><br/></pre></td></tr></table></figure>
<p>把 MySQL 驱动程序 https://dev.mysql.com/downloads/connector/j/ JAR
包复制到 <code>$HIVE_HOME/lib</code></p>
<p><code>hive-site.xml</code>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br/><span class="line">    JDBC connect string for a JDBC metastore.</span><br/><span class="line">    To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br/><span class="line">    For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.</span><br/><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>xxx<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">schematool -dbType mysql -initSchema</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br/><span class="line">use hive;</span><br/><span class="line">show tables;</span><br/><span class="line"><span class="keyword">select</span> * from DBS;</span><br/></pre></td></tr></table></figure>
<h3 id="启动-mysqldfsyarn-和-hiveserver2再用-beeline-连接-hs2">启动
MySQL、DFS、YARN 和 HiveServer2，再用 beeline 连接 HS2</h3>
<p>它的日志默认在 <code>${java.io.tmpdir}/yourname/hive.log</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/></pre></td><td class="code"><pre><span class="line">java -XshowSettings:properties -version</span><br/><span class="line">sudo systemctl start mysql.service</span><br/><span class="line">start-dfs.sh</span><br/><span class="line">start-yarn.sh</span><br/><span class="line">hiveserver2</span><br/><span class="line">beeline -u <span class="string">"jdbc:hive2://localhost:10000/;user=yourname"</span></span><br/><span class="line"><span class="built_in">help</span></span><br/></pre></td></tr></table></figure>
<p>运行建表脚本：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/&gt; !run /path/to/create_table1.hql</span><br/></pre></td></tr></table></figure>
<p>执行 HQL：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/&gt; show tables;</span><br/><span class="line">0: jdbc:hive2://localhost:10000/&gt; <span class="keyword">select</span> * from table1;</span><br/><span class="line">0: jdbc:hive2://localhost:10000/&gt; drop table table1;</span><br/></pre></td></tr></table></figure>
<h3 id="创建内部分区表">创建内部分区表</h3>
<p>它的 LOAD DATA INTO TABLE 操作会调用 MapReduce</p>
<p>加 LOCAL 是本地的，不加是 HDFS 的（？）</p>
<p>HQL 示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table1 (</span><br/><span class="line">  id STRING,</span><br/><span class="line">  name STRING,</span><br/><span class="line">  age <span class="type">INT</span>,</span><br/><span class="line">  department STRING</span><br/><span class="line">)</span><br/><span class="line">PARTITIONED <span class="keyword">BY</span> (city STRING)</span><br/><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br/><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'\t'</span></span><br/><span class="line">STORED <span class="keyword">AS</span> TEXTFILE;</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table1 <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (city<span class="operator">=</span><span class="string">'nanjing'</span>);</span><br/><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table1 <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (city<span class="operator">=</span><span class="string">'wuhan'</span>);</span><br/><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table1 <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (city<span class="operator">=</span><span class="string">'shanghai'</span>);</span><br/><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table1 <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (city<span class="operator">=</span><span class="string">'beijing'</span>);</span><br/><span class="line"></span><br/><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH <span class="string">'/home/rc/hive_test/data1.txt'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> table1;</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> table1;</span><br/></pre></td></tr></table></figure>
<p>文本文件示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/></pre></td><td class="code"><pre><span class="line">1	Alice	25	HR	nanjing</span><br/><span class="line">2	Bob	30	Finance	wuhan</span><br/><span class="line">3	Charlie	28	Sales	shanghai</span><br/><span class="line">4	David	35	Marketing	beijing</span><br/><span class="line">5	Eve	32	IT	nanjing</span><br/><span class="line">6	Frank	27	HR	wuhan</span><br/></pre></td></tr></table></figure>
<h3 id="创建有结构列的内部分区表">创建有结构列的内部分区表</h3>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table2 (</span><br/><span class="line">  id <span class="type">INT</span>,</span><br/><span class="line">  name STRING,</span><br/><span class="line">  hobbies <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span>,</span><br/><span class="line">  address_map MAP<span class="operator">&lt;</span>STRING, STRING<span class="operator">&gt;</span></span><br/><span class="line">)</span><br/><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br/><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">','</span></span><br/><span class="line">COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">';'</span></span><br/><span class="line">MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">':'</span>;</span><br/><span class="line"></span><br/><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH <span class="string">'/home/rc/hive_test/data2.txt'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> table2;</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> table2;</span><br/></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">1,Lilei,book;tv;code,beijing:chaoyang;shanghai:pudong</span><br/><span class="line">2,Hanmeimei,book;Lilei;code;basketball,beijing:haidian;shanghai:huangpu</span><br/></pre></td></tr></table></figure>
<h3 id="创建外部分桶表到-hbase用临时表数据覆写">创建外部分桶表到
HBase，用临时表数据覆写</h3>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> table3 (</span><br/><span class="line">    rowid STRING,</span><br/><span class="line">    name STRING,</span><br/><span class="line">    sex STRING,</span><br/><span class="line">    age <span class="type">INT</span></span><br/><span class="line">)</span><br/><span class="line">CLUSTERED <span class="keyword">BY</span> (rowid) <span class="keyword">INTO</span> <span class="number">5</span> BUCKETS</span><br/><span class="line">STORED <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br/><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (</span><br/><span class="line">    "hbase.columns.mapping" <span class="operator">=</span> ":key,cf:name,cf:sex,cf:age"</span><br/><span class="line">)</span><br/><span class="line">TBLPROPERTIES (</span><br/><span class="line">    "hbase.table.name" <span class="operator">=</span> "table3"</span><br/><span class="line">);</span><br/></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE table4 (</span><br/><span class="line">  rowid STRING,</span><br/><span class="line">  name STRING,</span><br/><span class="line">  department STRING,</span><br/><span class="line">  age INT</span><br/><span class="line">)</span><br/><span class="line">ROW FORMAT DELIMITED</span><br/><span class="line">FIELDS TERMINATED BY ':';</span><br/><span class="line"></span><br/><span class="line">LOAD DATA LOCAL INPATH '/home/rc/hive_test/data3.txt' INTO TABLE table4;</span><br/><span class="line"></span><br/><span class="line">INSERT OVERWRITE TABLE table3</span><br/><span class="line">SELECT * FROM table4;</span><br/><span class="line"></span><br/><span class="line">SELECT * FROM table3;</span><br/></pre></td></tr></table></figure>
<p>重启 Hive 客户端后，临时表 table4 会消失</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/></pre></td><td class="code"><pre><span class="line">1:John:Male:25</span><br/><span class="line">2:Smith:Female:30</span><br/><span class="line">3:Bob:Male:40</span><br/><span class="line">4:Alice:Female:22</span><br/><span class="line">5:Michael:Male:35</span><br/><span class="line">6:Emily:Female:28</span><br/></pre></td></tr></table></figure>
<h3 id="配置-5">配置</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: Error applying authorization policy on hive configuration: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span><br/><span class="line">	at org.apache.hive.service.cli.CLIService.init(CLIService.java:122) ~[hive-service-4.0.0.jar:4.0.0]</span><br/></pre></td></tr></table></figure>
<p>在 <code>hive-site.xml</code> 里把报错信息里报的开头的
<code>system</code> 去了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: xxx is not allowed to impersonate anonymous</span><br/><span class="line">	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:89) ~[hive-service-4.0.0.jar:4.0.0]</span><br/></pre></td></tr></table></figure>
<p><code>core-site.xml</code>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.xxx.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br/><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.xxx.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br/><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br/><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br/></pre></td></tr></table></figure>
<h2 id="flume-1.11.0">Flume 1.11.0</h2>
<p><img src="https://flume.apache.org/_images/DevGuide_image00.png"/></p>
<p>Flume 是一个水槽，用于采集、聚合和传输流数据（事件）。对于每一个代理
agent，有源 source、汇 sink、渠道 channel。</p>
<p>https://flume.apache.org/documentation.html</p>
<h3 id="启动参数">启动参数</h3>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf conf -f &lt;配置文件路径&gt; -n &lt;代理名&gt;</span><br/></pre></td></tr></table></figure>
<p>注意：下面的配置不一定对。笔者不小心把配置文件夹删了，但是保留的还有配置内容截图。所以下面的实际是
<a href="https://web.baimiaoapp.com/" rel="noopener" target="_blank">OCR</a>
过来后再修改的。之前是实验成功了的，而之后没有做过实验。</p>
<h3 id="hellonetcat-源与-logger-接收器">Hello（netcat 源与 logger
接收器）</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/></pre></td><td class="code"><pre><span class="line"># 代理名为 a1</span><br/><span class="line"># 以 s 结尾说明可以有多个 source、sink、channel</span><br/><span class="line">a1.sources = r1</span><br/><span class="line">a1.sinks = k1</span><br/><span class="line">a1.channels = c1</span><br/><span class="line"></span><br/><span class="line"># 设置 a1 的渠道 c1 为内存通道</span><br/><span class="line">a1.channels.c1.type = memory</span><br/><span class="line">a1.channels.c1.capacity = 1000</span><br/><span class="line">a1.channels.c1.transactionCapacity = 100</span><br/><span class="line"></span><br/><span class="line"># 把 a1 的源和汇 r1 和 k1 绑定到 a1 的渠道 c1 上</span><br/><span class="line"># source 可以指定多个渠道，sink 只能指定一个</span><br/><span class="line"># 以多种方式流向一个结果</span><br/><span class="line">a1.sources.r1.channels = c1</span><br/><span class="line">a1.sinks.k1.channel = c1</span><br/><span class="line"></span><br/><span class="line"># 设置 a1 的源 r1 为一个 netcat-like source</span><br/><span class="line"># 行为像 nc -lk [host] [port]</span><br/><span class="line"># 它侦听给定端口并将每行文本转换为一个事件</span><br/><span class="line">a1.sources.r1.type = netcat</span><br/><span class="line">a1.sources.r1.bind = localhost</span><br/><span class="line">a1.sources.r1.port = 44444</span><br/><span class="line"></span><br/><span class="line"># 设置 a1 的汇 k1 为 logger 类型，写到 flume.log 里面</span><br/><span class="line">a1.sinks.k1.type = logger</span><br/></pre></td></tr></table></figure>
<h3 id="实时监控单个追加文件exec-源与-hdfs-接收器">实时监控单个追加文件（exec
源与 hdfs 接收器）</h3>
<p>Exec Source 在启动时运行给定的 Unix
命令，并期望该进程在标准输出时连续生成数据。使用
<code>tail –F &lt;filename&gt;</code>
命令可以看到文件末尾的实时追加。</p>
<p>这里监控本机的 datanode 日志，并上传到 HDFS。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/><span class="line">33</span><br/></pre></td><td class="code"><pre><span class="line"># 代理</span><br/><span class="line">a2.sources = r2</span><br/><span class="line">a2.sinks = k2</span><br/><span class="line">a2.channels = c2</span><br/><span class="line"></span><br/><span class="line"># 渠道</span><br/><span class="line">a2.channels.c2.type = memory</span><br/><span class="line">a2.channels.c2.capacity = 1000</span><br/><span class="line">a2.channels.c2.transactionCapacity = 100</span><br/><span class="line"></span><br/><span class="line"># 源</span><br/><span class="line">a2.sources.r2.type = exec</span><br/><span class="line">a2.sources.r2.command = tail -F /opt/hadoop-ha/hadoop-3.3.6/logs/hadoop-rc-datanode-ubuntu101.log</span><br/><span class="line">a2.sources.r2.channels= c2</span><br/><span class="line"></span><br/><span class="line"># 汇</span><br/><span class="line">a2.sinks.k2.type = hdfs</span><br/><span class="line">a2.sinks.k2.hdfs.path = hdfs://mycluster/flume/%Y%m%d/%H</span><br/><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-</span><br/><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br/><span class="line"># 使用本地事件戳，把时间戳向下舍入，结合上面的配置是以小时作为子文件夹，即按小时分隔a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br/><span class="line">a2.sinks.k2.hdfs.round = true</span><br/><span class="line">a2.sinks.k2.hdfs.roundValue = 1</span><br/><span class="line">a2.sinks.k2.hdfs.roundUnit = hour</span><br/><span class="line"># 最多积攒多少个事件后，才将文件 flush 到 HDFS</span><br/><span class="line">a2.sinks.k2.hdfs.batchSize = 100</span><br/><span class="line"># 指定多少秒生成一个新的文件（滚动）</span><br/><span class="line">a2.sinks.k2.hdfs.rollInterval = 60</span><br/><span class="line"># 生成的文件最大大小（字节），略小于128M（HDFS的文件分块大小）</span><br/><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700</span><br/><span class="line"># 不指定滚动文件的事件数量</span><br/><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br/><span class="line">a2.sinks.k2.channel = c2</span><br/></pre></td></tr></table></figure>
<h3 id="实时监控多个新文件spooldir-源与-hdfs-接收器">实时监控多个新文件（spooldir
源与 hdfs 接收器）</h3>
<p>Spooling Directory Source
监视指定目录中的新文件，并在新文件出现时解析事件。</p>
<p>这里监控 <code>/opt/flume-1.11.0/upload</code>
目录。向目录里添加文件后，上传到 HDFS。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/><span class="line">33</span><br/><span class="line">34</span><br/><span class="line">35</span><br/><span class="line">36</span><br/><span class="line">37</span><br/><span class="line">38</span><br/><span class="line">39</span><br/><span class="line">40</span><br/></pre></td><td class="code"><pre><span class="line"># 代理</span><br/><span class="line">a3.sources = r3</span><br/><span class="line">a3.sinks = k3</span><br/><span class="line">a3.channels = c3</span><br/><span class="line"># 渠道</span><br/><span class="line">a3.channels.c3.type = memory</span><br/><span class="line">a3.channels.c3.capacity = 1000</span><br/><span class="line">a3.channels.c3.transactionCapacity = 100</span><br/><span class="line"># 源</span><br/><span class="line">a3.sources.r3.type = spooldir</span><br/><span class="line">a3.sources.r3.spoolDir = /opt/flume-1.11.0/upload</span><br/><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br/><span class="line"># 是否添加存储绝对路径文件名的标头</span><br/><span class="line">a3.sources.r3.fileHeader = true</span><br/><span class="line"># 忽略以.tmp 结尾的文件</span><br/><span class="line"># [^ ]*匹配任意不是空格的字符零次或多次</span><br/><span class="line">a3.sources.r3.ignorePattern = ^([^ ]*\.tmp)$</span><br/><span class="line"></span><br/><span class="line">a3.sources.r3.channels = c3</span><br/><span class="line"></span><br/><span class="line"># 汇</span><br/><span class="line">a3.sinks.k3.type = hdfs</span><br/><span class="line">a3.sinks.k3.hdfs.path = hdfs://mycluster/flume/upload/%Y%m%d/%H</span><br/><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br/><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br/><span class="line"># 使用本地事件戳，把时间戳向下舍入，结合上面的配置是以小时作为子文件夹，即按小时分隔</span><br/><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br/><span class="line">a3.sinks.k3.hdfs.round = true</span><br/><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br/><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br/><span class="line"># 最多积攒多少个事件后，才将文件 flush 到 HDFS</span><br/><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br/><span class="line"># 指定多少秒生成一个新的文件（滚动）</span><br/><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br/><span class="line"># 生成的文件最大大小（字节），略小于128M（HDFS的文件分块大小）</span><br/><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br/><span class="line"># 不指定滚动文件的事件数量</span><br/><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br/><span class="line"></span><br/><span class="line">a3.sinks.k3.channel = c3</span><br/></pre></td></tr></table></figure>
<h3 id="实时监控多个追加文件taildir-源与-hdfs-接收器">实时监控多个追加文件（TAILDIR
源与 hdfs 接收器）</h3>
<p>Taildir
Source：监视指定的文件，并在检测到附加到每个文件的新行后几乎实时地跟踪它们。如果正在写入新行，则此源将重试读取它们，等待写入完成。</p>
<p>监控 flume 目录里的 upload 目录和 upload1 目录。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/><span class="line">33</span><br/><span class="line">34</span><br/><span class="line">35</span><br/><span class="line">36</span><br/><span class="line">37</span><br/><span class="line">38</span><br/><span class="line">39</span><br/></pre></td><td class="code"><pre><span class="line"># 代理</span><br/><span class="line">a4.sources = r4</span><br/><span class="line">a4.sinks = k4</span><br/><span class="line">a4.channels = c4</span><br/><span class="line"># 渠道</span><br/><span class="line">a4.channels.c4.type = memory</span><br/><span class="line">a4.channels.c4.capacity = 1000</span><br/><span class="line">a4.channels.c4.transactionCapacity = 100</span><br/><span class="line"># 源</span><br/><span class="line">a4.sources.r4.type = TAILDIR</span><br/><span class="line"># JSON 格式的文件，用于记录每个尾部文件的inode、绝对路径和最后位置</span><br/><span class="line">a4.sources.r4.positionFile = /opt/flume-1.11.0/taildir_position.json</span><br/><span class="line">a4.sources.r4.filegroups = f1 f2</span><br/><span class="line"># 正则表达式只能用于文件名</span><br/><span class="line">a4.sources.r4.filegroups.f1 = /opt/flume-1.11.0/upload/.*</span><br/><span class="line">a4.sources.r4.filegroups.f2 = /opt/flume-1.11.0/upload1/.*</span><br/><span class="line">a4.sources.r4.channels = c4</span><br/><span class="line"></span><br/><span class="line"></span><br/><span class="line"># 汇</span><br/><span class="line">a4.sinks.k4.type = hdfs</span><br/><span class="line">a4.sinks.k4.hdfs.path = hdfs://mycluster/flume/upload/%Y%m%d/%H</span><br/><span class="line">a4.sinks.k4.hdfs.filePrefix = upload-</span><br/><span class="line">a4.sinks.k4.hdfs.fileType = DataStream</span><br/><span class="line"># 使用本地事件戳，把时间戳向下舍入，结合上面的配置是以小时作为子文件夹，即按小时分隔</span><br/><span class="line">a4.sinks.k4.hdfs.useLocalTimeStamp = true</span><br/><span class="line">a4.sinks.k4.hdfs.round = true</span><br/><span class="line">a4.sinks.k4.hdfs.roundValue = 1</span><br/><span class="line">a4.sinks.k4.hdfs.roundUnit = hour</span><br/><span class="line"># 最多积攒多少个事件后，才将文件 flush 到 HDFS</span><br/><span class="line">a4.sinks.k4.hdfs.batchSize = 100</span><br/><span class="line"># 指定多少秒生成一个新的文件（滚动）</span><br/><span class="line">a4.sinks.k4.hdfs.rollInterval = 20</span><br/><span class="line"># 生成的文件最大大小（字节），略小于128M（HDFS的文件分块大小）</span><br/><span class="line">a4.sinks.k4.hdfs.rollSize = 134217700</span><br/><span class="line"># 不指定滚动文件的事件数量</span><br/><span class="line">a4.sinks.k4.hdfs.rollCount = 0</span><br/><span class="line"></span><br/><span class="line">a4.sinks.k4.channel = c4</span><br/></pre></td></tr></table></figure>
<p>查看 <code>taildir_position.json</code>：其中 inode
号码是操作系统里文件的唯一 id，pos 是 flume
的读取到的最新的文件位置（偏移量）</p>
<p>Taildir source
是存在问题的：如果文件名变了，会重新上传。如果日志的文件名在一天过后变了，它会被重新上传一份。解决方案有修改
flume 的源码，或者修改生成日志文件名部分的源码。</p>
<h3 id="监控-mapreduce-结果上传到-hdfs">监控 MapReduce 结果，上传到
HDFS</h3>
<p>（1）使用 Flume 的 spooldir 源递归监控 <code>/opt/result/</code>
目录下的文件，汇总到 hdfs 接收器
<code>hdfs://mycluster/flume/mrresult</code>。</p>
<p>（2）将文献上传到 HDFS 的 <code>/wcinput</code> 目录，执行 MR
输出到本地路径 <code>file:///opt/result/mrresult</code></p>
<p>注意：如果提前建好 MR 的输出目录，MR 会报错。而如果不提前建好 flume
的监控目录，flume 会报错。</p>
<p>所以只提前建好外层目录，用 flume 递归监控外层目录，MR
输出到内层目录。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/><span class="line">33</span><br/><span class="line">34</span><br/><span class="line">35</span><br/><span class="line">36</span><br/><span class="line">37</span><br/><span class="line">38</span><br/><span class="line">39</span><br/><span class="line">40</span><br/><span class="line">41</span><br/><span class="line">42</span><br/><span class="line">43</span><br/><span class="line">44</span><br/><span class="line">45</span><br/></pre></td><td class="code"><pre><span class="line"># 代理</span><br/><span class="line">a5.sources = r5</span><br/><span class="line">a5.sinks = k5</span><br/><span class="line">a5.channels = c5</span><br/><span class="line"># 渠道</span><br/><span class="line">a5.channels.c5.type = memory</span><br/><span class="line">a5.channels.c5.capacity = 1000</span><br/><span class="line">a5.channels.c5.transactionCapacity = 100</span><br/><span class="line"># 源</span><br/><span class="line">a5.sources.r5.type = spooldir</span><br/><span class="line">a5.sources.r5.spoolDir = /opt/result</span><br/><span class="line"># 递归监视子目录</span><br/><span class="line">a5.sources.r5.recursiveDirectorySearch = true</span><br/><span class="line"># 指定文件名</span><br/><span class="line">a5.sources.r5.includePattern = ^part-r-00000$</span><br/><span class="line"></span><br/><span class="line">a5.sources.r5.fileSuffix = .COMPLETED</span><br/><span class="line"># 是否添加存储绝对路径文件名的标头</span><br/><span class="line">a5.sources.r5.fileHeader = true</span><br/><span class="line"># 忽略以.tmp 结尾的文件</span><br/><span class="line"># [^ ]*匹配任意不是空格的字符零次或多次</span><br/><span class="line">a5.sources.r5.ignorePattern = ^([^ ]*\.tmp)$</span><br/><span class="line"></span><br/><span class="line">a5.sources.r5.channels = c5</span><br/><span class="line"></span><br/><span class="line"># 汇</span><br/><span class="line">a5.sinks.k5.type = hdfs</span><br/><span class="line">a5.sinks.k5.hdfs.path = hdfs://mycluster/flume/mrresult</span><br/><span class="line">a5.sinks.k5.hdfs.filePrefix = upload-</span><br/><span class="line">a5.sinks.k5.hdfs.fileType = DataStream</span><br/><span class="line"># 使用本地事件戳，把时间戳向下舍入，结合上面的配置是以小时作为子文件夹，即按小时分隔</span><br/><span class="line">a5.sinks.k5.hdfs.useLocalTimeStamp = true</span><br/><span class="line">a5.sinks.k5.hdfs.round = true</span><br/><span class="line">a5.sinks.k5.hdfs.roundValue = 1</span><br/><span class="line">a5.sinks.k5.hdfs.roundUnit = hour</span><br/><span class="line"># 最多积攒多少个事件后，才将文件 flush 到 HDFS</span><br/><span class="line">a5.sinks.k5.hdfs.batchSize = 100</span><br/><span class="line"># 指定多少秒生成一个新的文件（滚动）</span><br/><span class="line">a5.sinks.k5.hdfs.rollInterval = 60</span><br/><span class="line"># 生成的文件最大大小（字节），略小于128M（HDFS的文件分块大小）</span><br/><span class="line">a5.sinks.k5.hdfs.rollSize = 134217700</span><br/><span class="line"># 不指定滚动文件的事件数量</span><br/><span class="line">a5.sinks.k5.hdfs.rollCount = 0</span><br/><span class="line"></span><br/><span class="line">a5.sinks.k5.channel = c5</span><br/></pre></td></tr></table></figure>
<p>执行 MR：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /wcinput file:///opt/result/mrresult</span><br/></pre></td></tr></table></figure>
<h2 id="超链接">超链接</h2>
<ul>
<li><a href="https://www.ruanyifeng.com/blog/2020/08/rsync.html" rel="noopener" target="_blank">rsync
用法教程</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-common/FileSystemShell.html" rel="noopener" target="_blank">FileSystem
Shell</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.3.6/api/org/apache/hadoop/fs/FileSystem.html" rel="noopener" target="_blank">FileSystem
API</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.3.6/api/org/apache/hadoop/fs/FileUtil.html" rel="noopener" target="_blank">FileUtil
API</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.3.6/api/org/apache/hadoop/fs/FileStatus.html" rel="noopener" target="_blank">FileStatus
API</a></li>
<li><a href="https://zookeeper.apache.org/doc/r3.8.2/zookeeperCLI.html" rel="noopener" target="_blank">zookeeper
CLI</a></li>
<li><a href="https://zookeeper.apache.org/doc/r3.8.2/apidocs/zookeeper-server/index.html" rel="noopener" target="_blank">zookeeper
Server API</a></li>
</ul>
<h2 id="持久化">持久化</h2>
<h3 id="命令-2">命令</h3>
<p>所谓使用命令，就是使用别人已经写好的软件。</p>
<p>环境变量 <code>PATH</code> 和物理路径，就是域名和 IP
地址的区别，要过一道中间商。域名和 IP 地址的中间商叫
DNS，<code>PATH</code> 和物理路径的中间商叫命令行解释器。</p>
<p>你输入一个命令，命令行解释器会在 <code>PATH</code>
里寻找你命令的源文件入口在哪，然后向它传递参数。</p>
<p>你可以使用 <code>which xxx</code> 或
<code>readlink -f $(which xxx)</code> 找你的命令源文件在哪里；使用
<code>which which</code> 找你的 <code>which</code> 在哪里。</p>
<p>查看虚拟机的 IP 地址：<code>ifconfig</code> <code>ip addr</code></p>
<h3 id="集群脚本">集群脚本</h3>
<p>前提是你搭了集群。笔者已经放弃集群了，太麻烦了。</p>
<p><strong>all：对集群的所有机器执行操作</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br/><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -eq 0 ]; <span class="keyword">then</span></span><br/><span class="line">    <span class="built_in">echo</span> <span class="string">"Error: Please provide at least one argument."</span></span><br/><span class="line">    <span class="built_in">exit</span> 1</span><br/><span class="line"><span class="keyword">else</span></span><br/><span class="line">    <span class="keyword">for</span> host <span class="keyword">in</span> ubuntu101 ubuntu102</span><br/><span class="line">    <span class="keyword">do</span></span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"============ <span class="variable">$host</span> ==========="</span></span><br/><span class="line">        ssh <span class="variable">$host</span> <span class="string">"<span class="variable">$@</span>"</span></span><br/><span class="line">    <span class="keyword">done</span></span><br/><span class="line"><span class="keyword">fi</span></span><br/></pre></td></tr></table></figure>
<p><strong>xsync：同步文件/目录到所有机器</strong></p>
<p>先测试从一台机器上同步到另一台机器上：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">rsync -av /path/to/src ubuntu102:/path/to/dst</span><br/></pre></td></tr></table></figure>
<p>从某台机器上同步到所有机器上：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br/><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br/><span class="line"><span class="keyword">then</span></span><br/><span class="line">    <span class="built_in">echo</span> Not Enough Argument!</span><br/><span class="line">    <span class="built_in">exit</span>;</span><br/><span class="line"><span class="keyword">fi</span></span><br/><span class="line"></span><br/><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> ubuntu101 ubuntu102 ubuntu103</span><br/><span class="line"><span class="keyword">do</span></span><br/><span class="line">    <span class="built_in">echo</span> ==================== <span class="variable">$host</span> ====================</span><br/><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> <span class="variable">$@</span></span><br/><span class="line">    <span class="keyword">do</span></span><br/><span class="line">        <span class="keyword">if</span> [ -e <span class="variable">$file</span> ]</span><br/><span class="line">        <span class="keyword">then</span></span><br/><span class="line">            pdir=$(<span class="built_in">cd</span> -P $(<span class="built_in">dirname</span> <span class="variable">$file</span>); <span class="built_in">pwd</span>)</span><br/><span class="line">            fname=$(<span class="built_in">basename</span> <span class="variable">$file</span>)</span><br/><span class="line">            ssh <span class="variable">$host</span> <span class="string">"mkdir -p <span class="variable">$pdir</span>"</span></span><br/><span class="line">            rsync -av <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$host</span>:<span class="variable">$pdir</span></span><br/><span class="line">        <span class="keyword">else</span></span><br/><span class="line">            <span class="built_in">echo</span> <span class="variable">$file</span> does not exists!</span><br/><span class="line">        <span class="keyword">fi</span></span><br/><span class="line">    <span class="keyword">done</span></span><br/><span class="line"><span class="keyword">done</span></span><br/></pre></td></tr></table></figure>
<p><strong>myhadoop</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br/><span class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></span><br/><span class="line">    <span class="string">"start"</span>)</span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"============ 启动 hadoop 集群 ============"</span></span><br/><span class="line"></span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"-------- 启动 HDFS --------"</span></span><br/><span class="line">        ssh <span class="string">"ubuntu101"</span> <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh"</span></span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"-------- 启动 YARN --------"</span></span><br/><span class="line">        ssh <span class="string">"ubuntu102"</span> <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh"</span></span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"-------- 启动 historyserver --------"</span></span><br/><span class="line">        ssh <span class="string">"ubuntu102"</span> <span class="string">"<span class="variable">$HADOOP_HOME</span>/bin/mapred --daemon start historyserver"</span></span><br/><span class="line"></span><br/><span class="line">        all jps</span><br/><span class="line">        ;;</span><br/><span class="line">    <span class="string">"stop"</span>)</span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"============ 关闭 hadoop 集群 ============"</span></span><br/><span class="line"></span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"-------- 关闭 historyserver --------"</span></span><br/><span class="line">        ssh <span class="string">"ubuntu102"</span> <span class="string">"<span class="variable">$HADOOP_HOME</span>/bin/mapred --daemon stop historyserver"</span></span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"-------- 关闭 YARN --------"</span></span><br/><span class="line">        ssh <span class="string">"ubuntu102"</span> <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/stop-yarn.sh"</span></span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"-------- 关闭 HDFS --------"</span></span><br/><span class="line">        ssh <span class="string">"ubuntu101"</span> <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/stop-dfs.sh"</span></span><br/><span class="line"></span><br/><span class="line">        all jps</span><br/><span class="line">        ;;</span><br/><span class="line">    *)</span><br/><span class="line">        <span class="built_in">echo</span> <span class="string">"Invalid parameter!"</span></span><br/><span class="line">        <span class="built_in">exit</span> 1</span><br/><span class="line">        ;;</span><br/><span class="line"><span class="keyword">esac</span></span><br/></pre></td></tr></table></figure>
<h2 id="搭建-hadoop-高可用集群">搭建 Hadoop 高可用集群</h2>
<p>HDFS 和 YARN
都是主从架构，当主节点挂了或者系统升级，集群会无法正常工作。高可用是指
7x24 小时系统可用，为此设置多个主节点。</p>
<p>对于 HDFS，主节点是
NameNode，它负责保存文件系统快照、操作日志、处理客户端读写请求，2NN
负责定期合并文件系统快照和操作日志。为实现高可用，设置多个 NameNode 和
JournalNode。同一时间只能有一个 NameNode 为 Active，它负责生成快照文件
FsImage，其他 NameNode 为 Standby，拉取同步 FsImage，还起到 2NN
的作用。JournalNode 负责保证 EditLog 的一致性。Zookeeper
负责监控集群，如果 Active 的 NameNode 挂了，通过 ZKFC 进行故障转移。</p>
<p>对于 YARN，主节点是 ResourceManager，从节点是
NodeManager。为此配置多个 ResourceManager。</p>
<p>https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</p>
<h2 id="hdfs-ha">HDFS-HA</h2>
<p>把原来的非高可用的 Hadoop
文件夹单独复制一份，重新写配置文件和环境变量、删除 data 和 logs
文件夹。先启动所有 journalnode 服务，再格式化一台机器的
namenode，启动该机器的 namenode 服务，然后在另两台机器上同步 namenode1
的元数据信息，并启动 namenode 服务。</p>
<h3 id="配置-6">配置</h3>
<p>三台机器上分别都配一个 NameNode、JournalNode、DataNode</p>
<p><code>core-site.xml</code>:</p>
<table>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>fs.defaultFS</td>
<td>hdfs://mycluster</td>
</tr>
<tr class="even">
<td>hadoop.tmp.dir</td>
<td>/usr/local/hadoop-ha/hadoop-3.3.6/data</td>
</tr>
<tr class="odd">
<td>hadoop.http.staticuser.user</td>
<td>rc</td>
</tr>
</tbody>
</table>
<p><code>hdfs-site.xml</code>:</p>
<p>注意 <code>dfs.journalnode.edits.dir</code> 不能以
<code>file://</code> 开头，前两个要以 <code>file://</code>
开头，不然报错。以及小心各种拼写错误。</p>
<table>
<colgroup>
<col style="width: 38%"/>
<col style="width: 61%"/>
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dfs.nameservices</td>
<td>mycluster</td>
</tr>
<tr class="even">
<td>dfs.namenode.name.dir</td>
<td><code>file://${hadoop.tmp.dir}/name</code></td>
</tr>
<tr class="odd">
<td>dfs.datanode.data.dir</td>
<td><code>file://${hadoop.tmp.dir}/data</code></td>
</tr>
<tr class="even">
<td>dfs.journalnode.edits.dir</td>
<td><code>${hadoop.tmp.dir}/journalnode</code></td>
</tr>
<tr class="odd">
<td>dfs.ha.namenodes.mycluster</td>
<td>namenode1,namenode2,namenode3</td>
</tr>
<tr class="even">
<td>dfs.namenode.rpc-address.mycluster.namenode1</td>
<td>ubuntu101:8020</td>
</tr>
<tr class="odd">
<td>dfs.namenode.rpc-address.mycluster.namenode2</td>
<td>ubuntu102:8020</td>
</tr>
<tr class="even">
<td>dfs.namenode.rpc-address.mycluster.namenode3</td>
<td>ubuntu103:8020</td>
</tr>
<tr class="odd">
<td>dfs.namenode.http-address.mycluster.namenode1</td>
<td>ubuntu101:9870</td>
</tr>
<tr class="even">
<td>dfs.namenode.http-address.mycluster.namenode2</td>
<td>ubuntu102:9870</td>
</tr>
<tr class="odd">
<td>dfs.namenode.http-address.mycluster.namenode3</td>
<td>ubuntu103:9870</td>
</tr>
<tr class="even">
<td>dfs.namenode.shared.edits.dir</td>
<td>qjournal://ubuntu101:8485;ubuntu102:8485;ubuntu103:8485/mycluster</td>
</tr>
<tr class="odd">
<td>dfs.client.failover.proxy.provider.mycluster</td>
<td>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</td>
</tr>
<tr class="even">
<td>dfs.ha.fencing.methods</td>
<td>sshfence</td>
</tr>
<tr class="odd">
<td>dfs.ha.fencing.ssh.private-key-files</td>
<td>/home/rc/.ssh/id_rsa</td>
</tr>
</tbody>
</table>
<h3 id="实验">实验</h3>
<p>在每一台机器上启动 JournalNode 服务：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs --daemon start journalnode</span><br/></pre></td></tr></table></figure>
<p>在 101 机器上对 namenode 进行格式化，并启动 namenode：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br/><span class="line">hdfs --daemon start namenode</span><br/></pre></td></tr></table></figure>
<p>在 Web 界面查看，为 Standby。</p>
<p>在另两台机器上同步 namenode1 的元数据信息，并启动 namenode：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby</span><br/><span class="line">hdfs --daemon start namenode</span><br/></pre></td></tr></table></figure>
<p>在每一台机器上启动 DataNode：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br/></pre></td></tr></table></figure>
<p>把 namenode2 切换成 Active 状态：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs haadmin -transitionToActive namenode2</span><br/></pre></td></tr></table></figure>
<p>模拟 namenode2 挂掉：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 &lt;进程号&gt;</span><br/></pre></td></tr></table></figure>
<p>这时 namenode1 和 namenode3 还是 Standby。如果手动激活某一个，会显示
102 拒绝连接。</p>
<p>重启 namenode2 后，三个 namenode 还是
Standby。这时再激活某一个，激活成功。这说明当所有的 namenode
都启动成功时，才可以激活某一个
namenode。这失去了高可用的意义和作用。为什么会这样呢？因为我们前面配置了隔离机制，同一时刻只能有一台
Active 的 namenode 响应客户端。如果有 namenode 挂了，其他 namenode
只是联系不上它，不知道是不是真的挂了。如果它没挂且是
Active，再激活其他机器，会出现两台
Active。为了准确无误地知道它是否挂了，需要配置 ZooKeeper 监控集群。</p>
<h3 id="自动故障转移配置">自动故障转移配置</h3>
<p>在三台机器上都加一个 Zookeeper 和 ZKFC。</p>
<p>在上面的配置文件基础上增加。</p>
<p><code>hdfs-site.xml</code>:</p>
<table>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dfs.ha.automatic-failover.enabled</td>
<td>true</td>
</tr>
</tbody>
</table>
<p><code>core-site.xml</code>:</p>
<p>端口号要与 ZooKeeper 配置文件里的一致。</p>
<table>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ha.zookeeper.quorum</td>
<td>ubuntu101:2181,ubuntu102:2181,ubuntu103:2181</td>
</tr>
</tbody>
</table>
<h3 id="自动故障转移实验">自动故障转移实验</h3>
<p>必须在 stop-dfs 之后，并启动 ZooKeeper
集群成功后，再在任意一台机器上初始化 HA 在 Zookeeper 中状态。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs zkfc –formatZK</span><br/></pre></td></tr></table></figure>
<p>然后 start-dfs。formatZK 成功后，以后启动集群需要先启动 ZK
服务端，后启动 dfs。如果先启动 dfs，这时会有 ZKFC 进程，再启动 ZK
服务端后，ZKFC 进程被挤掉了，所有 namenode 都是 Standby。</p>
<p>查看当前活跃节点：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs haadmin -getAllServiceState</span><br/></pre></td></tr></table></figure>
<p>或者在 ZK 客户端查看选举锁：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">get -s /hadoop-ha/mycluster/ActiveStandbyElectorLock</span><br/></pre></td></tr></table></figure>
<p>验证集群会不会进行故障转移：kill 掉 Active 的 namenode</p>
<h2 id="yarn-ha">YARN-HA</h2>
<h3 id="配置-7">配置</h3>
<p>三台机器上分别都配一个 ResourceManager、NodeManager、ZooKeeper</p>
<p><code>yarn-site.xml</code>:</p>
<table>
<colgroup>
<col style="width: 28%"/>
<col style="width: 71%"/>
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>yarn.nodemanager.aux-services</td>
<td>mapreduce_shuffle</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.ha.enabled</td>
<td>true</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.recovery.enabled</td>
<td>true</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.store.class</td>
<td>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.zk-address</td>
<td>ubuntu101:2181,ubuntu102:2181,ubuntu103:2181</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.cluster-id</td>
<td>cluster-yarn1</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.ha.rm-ids</td>
<td>rm1,rm2,rm3</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.hostname.rm1</td>
<td>ubuntu101</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.hostname.rm2</td>
<td>ubuntu102</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.hostname.rm3</td>
<td>ubuntu103</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.webapp.address.rm1</td>
<td>ubuntu101:8088</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.webapp.address.rm2</td>
<td>ubuntu102:8088</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.webapp.address.rm3</td>
<td>ubuntu103:8088</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.address.rm1</td>
<td>ubuntu101:8032</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.address.rm2</td>
<td>ubuntu102:8032</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.address.rm3</td>
<td>ubuntu103:8032</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.scheduler.address.rm1</td>
<td>ubuntu101:8030</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.scheduler.address.rm2</td>
<td>ubuntu102:8030</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.scheduler.address.rm3</td>
<td>ubuntu103:8030</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.resource-tracker.address.rm1</td>
<td>ubuntu101:8031</td>
</tr>
<tr class="odd">
<td>yarn.resourcemanager.resource-tracker.address.rm2</td>
<td>ubuntu102:8031</td>
</tr>
<tr class="even">
<td>yarn.resourcemanager.resource-tracker.address.rm3</td>
<td>ubuntu103:8031</td>
</tr>
<tr class="odd">
<td>yarn.nodemanager.env-whitelist</td>
<td>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</td>
</tr>
</tbody>
</table>
<h3 id="实验-1">实验</h3>
<p>查看当前活跃节点：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">yarn rmadmin -getAllServiceState</span><br/></pre></td></tr></table></figure>
<p>如果浏览器访问 Standby 节点的 8088 端口（RM），会自动跳转到 Active
节点。</p>
<h2 id="解决-jsch-认证失败">解决 JSch 认证失败</h2>
<p>问题出现在配置 HDFS HA 自动故障转移时。杀掉活跃的 NN
之后，它没有被隔离成功。</p>
<p>JSch 是一个库，它在 Java 程序里建立 SSH 连接。</p>
<h3 id="在杀掉-acitive-的-nn-过程中">在杀掉 Acitive 的 NN 过程中</h3>
<p>被杀的 Acitive 的 NN 上的 ZKFC 日志：</p>
<ol type="1">
<li>[08:38:38,534] hadoop 高可用健康监测者抛出 EOF 异常，进入
SERVICE_NOT_RESPONDING 状态</li>
<li>[08:38:38,615]
<ul>
<li>org.apache.hadoop.hdfs.tools.DFSZKFailoverController: 获取不到本地
NN 的线程转储，由于连接被拒绝</li>
<li>org.apache.hadoop.ha.ZKFailoverController: 退出 NN
的主选举，并标记需要隔离</li>
<li>hadoop 高可用激活/备用选举者开始重新选举</li>
</ul></li>
<li>[08:38:38,636] ZK
客户端不能从服务端读取会话的附加信息，说服务端好像把套接字关闭了</li>
<li>[08:38:38,739] 会话被关闭，ZK 客户端上对应的事件线程被终止</li>
<li>之后 hadoop 高可用健康监测者一直尝试重新连接 NN，连不上</li>
</ol>
<p>原来 Standby 的 NN 上的 ZKFC 日志：</p>
<ol type="1">
<li>[08:38:38,719] 选举者检查到了需要被隔离的原活跃节点，ZKFC
找到了隔离目标</li>
<li>[08:38:39,738] org.apache.hadoop.ha.FailoverController
联系不上被杀的 NN</li>
<li>[08:38:39,748] 高可用节点隔离者开始隔离，用
org.apache.hadoop.ha.SshFenceByTcpPort，里面用了 JSch
库建立客户端（本机）与服务端（被杀的）之间的 SSH 连接</li>
<li>[08:38:40,113] SSH 认证失败，隔离方法没有成功，选举失败</li>
<li>之后选举者一直在重建 ZK 连接，重新连 NN 连不上，重新隔离失败</li>
</ol>
<h3 id="软件版本">软件版本</h3>
<p>客户端（Standby 上的
org.apache.hadoop.ha.SshFenceByTcpPort.jsch）：</p>
<ul>
<li>Hadoop 3.3.6 <a href="https://github.com/apache/hadoop/blob/branch-3.3.6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java" rel="noopener" target="_blank">SshFenceByTcpPort
源码</a></li>
<li>JSch 0.1.55</li>
</ul>
<p>服务端（被杀的）：</p>
<ul>
<li>OpenSSH_8.9p1 Ubuntu-3ubuntu0.6, OpenSSL 3.0.2 15 Mar 2022 里的
sshd</li>
</ul>
<p>生成密钥时用的命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -m PEM</span><br/></pre></td></tr></table></figure>
<h3 id="关键日志">关键日志</h3>
<p>原来为 Standby 的 NN 上的 ZKFC 日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/></pre></td><td class="code"><pre><span class="line">INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch:</span><br/><span class="line">//...</span><br/><span class="line">Remote version string: SSH-2.0-OpenSSH_8.9p1 Ubuntu-3ubuntu0.6</span><br/><span class="line">Local version string: SSH-2.0-JSCH-0.1.54</span><br/><span class="line">// JSch 0.1.55 的源码里，写的字符串就是 0.1.54，这个应该不影响</span><br/><span class="line">//...</span><br/><span class="line">Authentications that can continue: publickey,keyboard-interactive,password</span><br/><span class="line">Next authentication method: publickey</span><br/><span class="line">// 这里用公钥认证失败了</span><br/><span class="line">Authentications that can continue: password</span><br/><span class="line">Next authentication method: password</span><br/><span class="line">Disconnecting from ubuntu103 port 22</span><br/><span class="line">WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to connect to ubuntu103 as user rc</span><br/><span class="line">com.jcraft.jsch.JSchException: Auth fail</span><br/></pre></td></tr></table></figure>
<h3 id="解决思路">解决思路</h3>
<p>看 sshd 的日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line">userauth_pubkey: key type ssh-rsa not in PubkeyAcceptedAlgorithms</span><br/><span class="line">error: Received disconnect from 192.168.78.101 port 49968:3: com.jcraft.jsch.JSchException: Auth fail</span><br/><span class="line">Disconnected from authenticating user rc 192.168.78.101 port 49968</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">sudo vi /etc/ssh/sshd_config</span><br/></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">PubkeyAuthentication yes</span><br/><span class="line">PubkeyAcceptedAlgorithms +ssh-rsa</span><br/></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">sudo systemctl restart sshd</span><br/></pre></td></tr></table></figure>
<h2 id="不是问题的问题">不是问题的问题</h2>
<p>用</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br/></pre></td><td class="code"><pre><span class="line">hdfs haadmin -getAllServiceState</span><br/></pre></td></tr></table></figure>
<p>查看 Active 转移成功了，但是联系不上被杀的那一方。</p>
<p>具体地说，转移成功之后，三个方的 DN 都一直在尝试连被杀那一方的
NN，一直在写日志。除此之外上传下载都没问题。</p>
<p>这应该是集群自带的心跳机制，不是问题。</p>
</div>
<footer class="post-footer">
<div class="post-nav">
<div class="post-nav-item">
<a href="/blog/python.html" rel="prev" title="Python">
<i class="fa fa-chevron-left"></i> Python
                </a>
</div>
<div class="post-nav-item">
<a href="/blog/myclock.html" rel="next" title="用C语言模拟一个像素时钟">
                  用C语言模拟一个像素时钟 <i class="fa fa-chevron-right"></i>
</a>
</div>
</div>
</footer>
</article>
</div>
</div>
</main>
<footer class="footer">
<div class="footer-inner">
<div class="copyright">
  2023 – 
  <span itemprop="copyrightYear">2024</span>
<span class="with-love">
<i class="fa fa-heart"></i>
</span>
<span class="author" itemprop="copyrightHolder">Ruofan</span>
</div>
</div>
</footer>
<div aria-label="返回顶部" class="back-to-top" role="button">
<i class="fa fa-arrow-up fa-lg"></i>
<span>0%</span>
</div>
<div class="reading-progress-bar"></div>
<noscript>
<div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script crossorigin="anonymous" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script>
<script crossorigin="anonymous" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js"></script>
<script src="/blog/js/comments.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/next-boot.js"></script>
<script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
<script src="/blog/js/third-party/tags/mermaid.js"></script>
<script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/blog/js/third-party/math/mathjax.js"></script>
<script crossorigin="anonymous" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js"></script>
<script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://hrfis.me/blog/hadoop.html"}</script>
<script src="/blog/js/third-party/quicklink.js"></script>
</body>
</html>
